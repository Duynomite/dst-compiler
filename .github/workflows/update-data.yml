name: Update DST Data

on:
  schedule:
    - cron: '0 11 * * *'    # 6:00 AM EST (11:00 UTC) daily
    - cron: '0 15 * * 1'    # Monday 10:00 AM EST (15:00 UTC) — weekly URL check
  workflow_dispatch:          # Manual trigger from Actions tab

jobs:
  update-data:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      issues: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run data fetcher
        id: fetcher
        run: python dst_data_fetcher.py 2>&1 | tee fetcher_output.txt
        timeout-minutes: 10

      - name: Check for coverage gaps (weekly only)
        id: coverage-gaps
        if: github.event.schedule == '0 15 * * 1' || github.event_name == 'workflow_dispatch'
        run: |
          if grep -q "FEMA→STATE gap\|FMCSA→STATE gap" fetcher_output.txt; then
            echo "Coverage gaps detected:"
            grep "gap:" fetcher_output.txt
            echo "has_gaps=true" >> "$GITHUB_OUTPUT"
          else
            echo "No coverage gaps"
            echo "has_gaps=false" >> "$GITHUB_OUTPUT"
          fi
        continue-on-error: true

      - name: Validate record counts (total + per-source)
        id: validate
        run: |
          python3 << 'PYEOF'
          import json, subprocess, sys

          # Load new data
          new = json.load(open('curated_disasters.json'))
          new_records = new.get('disasters', []) if isinstance(new, dict) else new
          new_total = len(new_records)
          new_by_source = {}
          for r in new_records:
              src = r.get('source', 'UNKNOWN')
              new_by_source[src] = new_by_source.get(src, 0) + 1
          print(f'New data: {new_total} total, by source: {new_by_source}')

          # Load old data from git
          try:
              old_json = subprocess.check_output(
                  ['git', 'show', 'HEAD:curated_disasters.json'],
                  stderr=subprocess.DEVNULL
              ).decode()
              old = json.loads(old_json)
              old_records = old.get('disasters', []) if isinstance(old, dict) else old
              old_total = len(old_records)
              old_by_source = {}
              for r in old_records:
                  src = r.get('source', 'UNKNOWN')
                  old_by_source[src] = old_by_source.get(src, 0) + 1
          except Exception:
              old_by_source = {}
              old_total = 0
          print(f'Old data: {old_total} total, by source: {old_by_source}')

          errors = []

          # Check 1: Total count must not drop
          if new_total < old_total:
              errors.append(f'Total count DROPPED from {old_total} to {new_total} (-{old_total - new_total})')

          # Check 2: Per-source integrity
          for src, old_count in old_by_source.items():
              new_count = new_by_source.get(src, 0)
              # Any source dropping to 0 from non-zero = blocked
              if old_count > 0 and new_count == 0:
                  errors.append(f'{src}: dropped from {old_count} to 0 (total loss)')
              # Source with 5+ records losing >20% = blocked
              elif old_count >= 5 and new_count < old_count * 0.8:
                  pct = (1 - new_count / old_count) * 100
                  errors.append(f'{src}: dropped from {old_count} to {new_count} ({pct:.0f}% loss)')

          if errors:
              for e in errors:
                  print(f'::error::{e}')
              print('\nThis likely means the Federal Register API returned fewer results in CI.')
              print('Run the fetcher locally and push manually to restore full data.')
              sys.exit(1)

          print('Record count checks PASSED')
          PYEOF

      - name: Run data integrity audit (25 checks per record)
        id: audit
        run: python audit_curated_data.py --ci
        timeout-minutes: 2

      - name: Verify URLs (weekly only)
        id: url-check
        if: github.event.schedule == '0 15 * * 1' || github.event_name == 'workflow_dispatch'
        run: python audit_curated_data.py --verify-urls --update-metadata
        timeout-minutes: 5
        continue-on-error: true

      - name: Check eCFR for regulatory changes (weekly only)
        id: ecfr-check
        if: github.event.schedule == '0 15 * * 1' || github.event_name == 'workflow_dispatch'
        run: python audit_curated_data.py --check-ecfr --update-metadata
        timeout-minutes: 2
        continue-on-error: true

      - name: Commit and push if data changed
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git diff --quiet curated_disasters.json || (
            git add curated_disasters.json
            git commit -m "Auto-update curated disaster data"
            git push
          )

      - name: Create issue on data loss
        if: failure() && steps.validate.outcome == 'failure'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'DST Data Loss Detected — Per-Source Record Drop',
              body: `The daily data update detected a significant record count drop by source.\n\nThis likely means the Federal Register API returned fewer results than expected, or curated data was corrupted.\n\n**Action required:** Run the fetcher locally and push manually.\n\nSee the failed run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
              labels: ['data-integrity', 'urgent']
            });

      - name: Create issue on audit failure
        if: failure() && steps.audit.outcome == 'failure'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'DST Audit Failed — Data Integrity Check',
              body: `The 22-check data integrity audit found failures in curated_disasters.json.\n\nRecords may have invalid dates, missing fields, or incorrect SEP calculations.\n\n**Action required:** Run \`python3 audit_curated_data.py\` locally to see details.\n\nSee the failed run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
              labels: ['data-integrity', 'audit']
            });

      - name: Create issue on URL verification failure
        if: steps.url-check.outcome == 'failure'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'DST URL Verification — Dead Links Detected',
              body: `The weekly URL verification found officialUrl entries that are unreachable or point to incorrect pages.\n\n**Action required:** Run \`python3 audit_curated_data.py --verify-urls\` locally to see which URLs failed.\n\nSee the run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
              labels: ['data-integrity', 'urls']
            });

      - name: Create issue on regulatory change detected
        if: steps.ecfr-check.outcome == 'failure'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: '⚠️ REGULATORY CHANGE — 42 CFR § 422.62 Amended',
              body: `The weekly eCFR regulatory monitor detected that **42 CFR § 422.62** (Election of coverage under an MA plan) has been amended.\n\nThis regulation defines DST Special Enrollment Periods. If the SEP window duration (2 months) or maximum ongoing period (14 months) changed, **our SEP calculations may be WRONG**.\n\n**Action required (URGENT):**\n1. Review the amended regulation: https://www.ecfr.gov/current/title-42/chapter-IV/subchapter-B/part-422/subpart-B/section-422.62\n2. Check if SEP window duration changed\n3. Check if qualifying declaration types changed\n4. Update SEP calculations in index.html and dst_data_fetcher.py\n5. Update ECFR_KNOWN_VERSION_DATE in audit_curated_data.py\n\nSee the run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
              labels: ['data-integrity', 'regulatory', 'urgent']
            });

      - name: Create issue on coverage gaps detected
        if: steps.coverage-gaps.outputs.has_gaps == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const output = fs.readFileSync('fetcher_output.txt', 'utf8');
            const gapLines = output.split('\n').filter(l => l.includes('gap:'));
            const gapList = gapLines.map(l => `- ${l.trim()}`).join('\n');
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'DST Coverage Gap — Missing Governor Declarations',
              body: `The weekly coverage gap analysis found states with FEMA or FMCSA disasters but **no curated governor declaration**.\n\n## Gaps Detected\n${gapList}\n\n## Why This Matters\nGovernor emergency declarations independently trigger Medicare DST Special Enrollment Periods. Without them, agents may miss valid SEP opportunities for beneficiaries in affected counties.\n\n## Action Required\n1. Research governor emergency declarations for each flagged state\n2. Find the official proclamation URL on the governor's website\n3. Add to \`StateCollector._get_curated_state()\` in \`dst_data_fetcher.py\`\n4. Run \`python dst_data_fetcher.py && python audit_curated_data.py\` to validate\n5. Push to main\n\nSee the run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
              labels: ['data-integrity', 'coverage-gap']
            });
