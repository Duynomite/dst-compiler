name: Update DST Data

on:
  schedule:
    - cron: '0 11 * * *'    # 6:00 AM EST (11:00 UTC) daily
    - cron: '0 15 * * 1'    # Monday 10:00 AM EST (15:00 UTC) — weekly URL check
    - cron: '0 12 1 * *'    # 1st of each month 7:00 AM EST (12:00 UTC) — monthly enrollment refresh
  workflow_dispatch:          # Manual trigger from Actions tab

jobs:
  update-data:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      issues: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run data fetcher
        id: fetcher
        run: python dst_data_fetcher.py 2>&1 | tee fetcher_output.txt
        timeout-minutes: 15

      - name: Refresh Medicare enrollment data (monthly only)
        id: enrollment
        if: github.event.schedule == '0 12 1 * *' || github.event_name == 'workflow_dispatch'
        run: python build_medicare_enrollment.py
        timeout-minutes: 5
        continue-on-error: true

      - name: Check for coverage gaps (weekly only)
        id: coverage-gaps
        if: github.event.schedule == '0 15 * * 1' || github.event_name == 'workflow_dispatch'
        run: |
          if grep -q "FEMA→STATE gap\|FMCSA→STATE gap" fetcher_output.txt; then
            echo "Coverage gaps detected:"
            grep "gap:" fetcher_output.txt
            echo "has_gaps=true" >> "$GITHUB_OUTPUT"
          else
            echo "No coverage gaps"
            echo "has_gaps=false" >> "$GITHUB_OUTPUT"
          fi
        continue-on-error: true

      - name: Validate record counts (total + per-source)
        id: validate
        run: |
          python3 << 'PYEOF'
          import json, subprocess, sys, os

          def validate_file(filename, allow_fema_zero=False):
              """Validate record counts for a JSON file. Returns list of errors."""
              if not os.path.exists(filename):
                  return [f'{filename} not found']

              new = json.load(open(filename))
              new_records = new.get('disasters', []) if isinstance(new, dict) else new
              new_total = len(new_records)
              new_by_source = {}
              for r in new_records:
                  src = r.get('source', 'UNKNOWN')
                  new_by_source[src] = new_by_source.get(src, 0) + 1
              print(f'{filename}: {new_total} total, by source: {new_by_source}')

              # Load old data from git
              try:
                  old_json = subprocess.check_output(
                      ['git', 'show', f'HEAD:{filename}'],
                      stderr=subprocess.DEVNULL
                  ).decode()
                  old = json.loads(old_json)
                  old_records = old.get('disasters', []) if isinstance(old, dict) else old
                  old_total = len(old_records)
                  old_by_source = {}
                  for r in old_records:
                      src = r.get('source', 'UNKNOWN')
                      old_by_source[src] = old_by_source.get(src, 0) + 1
              except Exception:
                  old_by_source = {}
                  old_total = 0
              print(f'{filename} (previous): {old_total} total, by source: {old_by_source}')

              errors = []

              # Check 1: Total count must not drop (for curated; all_disasters may vary with FEMA)
              if not allow_fema_zero and new_total < old_total:
                  errors.append(f'{filename}: Total count DROPPED from {old_total} to {new_total}')

              # Check 2: Per-source integrity
              for src, old_count in old_by_source.items():
                  new_count = new_by_source.get(src, 0)
                  # FEMA source: warn but don't block if 0 (API could be temporarily down)
                  if src == 'FEMA' and allow_fema_zero:
                      if old_count > 0 and new_count == 0:
                          print(f'::warning::{filename}: FEMA dropped from {old_count} to 0 (API may be down)')
                      continue
                  # Any non-FEMA source dropping to 0 from non-zero = blocked
                  if old_count > 0 and new_count == 0:
                      errors.append(f'{filename}: {src} dropped from {old_count} to 0 (total loss)')
                  # Source with 5+ records losing >20% = blocked
                  elif old_count >= 5 and new_count < old_count * 0.8:
                      pct = (1 - new_count / old_count) * 100
                      errors.append(f'{filename}: {src} dropped from {old_count} to {new_count} ({pct:.0f}% loss)')

              return errors

          all_errors = []

          # Validate curated_disasters.json (strict — no FEMA expected)
          all_errors.extend(validate_file('curated_disasters.json'))

          # Validate all_disasters.json (FEMA=0 is a warning, not a blocker)
          all_errors.extend(validate_file('all_disasters.json', allow_fema_zero=True))

          if all_errors:
              for e in all_errors:
                  print(f'::error::{e}')
              print('\nRecord count validation failed.')
              print('Run the fetcher locally and push manually to restore full data.')
              sys.exit(1)

          print('Record count checks PASSED for both files')
          PYEOF

      - name: Run data integrity audit — curated_disasters.json
        id: audit
        run: python audit_curated_data.py --ci
        timeout-minutes: 2

      - name: Run data integrity audit — all_disasters.json
        id: audit-all
        run: python audit_curated_data.py --ci --all-disasters --json-path all_disasters.json
        timeout-minutes: 2

      - name: Verify URLs (weekly only)
        id: url-check
        if: github.event.schedule == '0 15 * * 1' || github.event_name == 'workflow_dispatch'
        run: python audit_curated_data.py --verify-urls --update-metadata
        timeout-minutes: 5
        continue-on-error: true

      - name: Check eCFR for regulatory changes (weekly only)
        id: ecfr-check
        if: github.event.schedule == '0 15 * * 1' || github.event_name == 'workflow_dispatch'
        run: python audit_curated_data.py --check-ecfr --update-metadata
        timeout-minutes: 2
        continue-on-error: true

      - name: Commit and push if data changed
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add curated_disasters.json all_disasters.json medicare_enrollment.json
          git diff --cached --quiet || (
            git commit -m "Auto-update disaster + enrollment data"
            git push
          )

      - name: Create issue on data loss
        if: failure() && steps.validate.outcome == 'failure'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'DST Data Loss Detected — Per-Source Record Drop',
              body: `The daily data update detected a significant record count drop by source.\n\nThis likely means the Federal Register API returned fewer results than expected, or curated data was corrupted.\n\n**Action required:** Run the fetcher locally and push manually.\n\nSee the failed run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
              labels: ['data-integrity', 'urgent']
            });

      - name: Create issue on audit failure
        if: failure() && (steps.audit.outcome == 'failure' || steps.audit-all.outcome == 'failure')
        uses: actions/github-script@v7
        with:
          script: |
            const curatedFailed = '${{ steps.audit.outcome }}' === 'failure';
            const allFailed = '${{ steps.audit-all.outcome }}' === 'failure';
            const files = [];
            if (curatedFailed) files.push('curated_disasters.json');
            if (allFailed) files.push('all_disasters.json');
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `DST Audit Failed — ${files.join(' + ')}`,
              body: `The data integrity audit found failures in ${files.join(' and ')}.\n\nRecords may have invalid dates, missing fields, or incorrect SEP calculations.\n\n**Action required:** Run the audit locally to see details:\n\`\`\`\npython3 audit_curated_data.py --ci\npython3 audit_curated_data.py --ci --all-disasters --json-path all_disasters.json\n\`\`\`\n\nSee the failed run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
              labels: ['data-integrity', 'audit']
            });

      - name: Create issue on URL verification failure
        if: steps.url-check.outcome == 'failure'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'DST URL Verification — Dead Links Detected',
              body: `The weekly URL verification found officialUrl entries that are unreachable or point to incorrect pages.\n\n**Action required:** Run \`python3 audit_curated_data.py --verify-urls\` locally to see which URLs failed.\n\nSee the run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
              labels: ['data-integrity', 'urls']
            });

      - name: Create issue on regulatory change detected
        if: steps.ecfr-check.outcome == 'failure'
        uses: actions/github-script@v7
        with:
          script: |
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: '⚠️ REGULATORY CHANGE — 42 CFR § 422.62 Amended',
              body: `The weekly eCFR regulatory monitor detected that **42 CFR § 422.62** (Election of coverage under an MA plan) has been amended.\n\nThis regulation defines DST Special Enrollment Periods. If the SEP window duration (2 months) or maximum ongoing period (14 months) changed, **our SEP calculations may be WRONG**.\n\n**Action required (URGENT):**\n1. Review the amended regulation: https://www.ecfr.gov/current/title-42/chapter-IV/subchapter-B/part-422/subpart-B/section-422.62\n2. Check if SEP window duration changed\n3. Check if qualifying declaration types changed\n4. Update SEP calculations in index.html and dst_data_fetcher.py\n5. Update ECFR_KNOWN_VERSION_DATE in audit_curated_data.py\n\nSee the run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
              labels: ['data-integrity', 'regulatory', 'urgent']
            });

      - name: Create issue on coverage gaps detected
        if: steps.coverage-gaps.outputs.has_gaps == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const output = fs.readFileSync('fetcher_output.txt', 'utf8');
            const gapLines = output.split('\n').filter(l => l.includes('gap:'));
            const gapList = gapLines.map(l => `- ${l.trim()}`).join('\n');
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'DST Coverage Gap — Missing Governor Declarations',
              body: `The weekly coverage gap analysis found states with FEMA or FMCSA disasters but **no curated governor declaration**.\n\n## Gaps Detected\n${gapList}\n\n## Why This Matters\nGovernor emergency declarations independently trigger Medicare DST Special Enrollment Periods. Without them, agents may miss valid SEP opportunities for beneficiaries in affected counties.\n\n## Action Required\n1. Research governor emergency declarations for each flagged state\n2. Find the official proclamation URL on the governor's website\n3. Add to \`StateCollector._get_curated_state()\` in \`dst_data_fetcher.py\`\n4. Run \`python dst_data_fetcher.py && python audit_curated_data.py\` to validate\n5. Push to main\n\nSee the run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
              labels: ['data-integrity', 'coverage-gap']
            });
